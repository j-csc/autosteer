# -*- coding: utf-8 -*-
"""steer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vk3C2vYRV7VlbNK1kp_jUAKOFexsz79u
"""

# Commented out IPython magic to ensure Python compatibility.
try:
    import google.colab  # type: ignore
    from google.colab import output

    COLAB = True
#     %pip install sae-lens transformer-lens sae-dashboard
except:
    COLAB = False
    from IPython import get_ipython  # type: ignore

    ipython = get_ipython()
    assert ipython is not None
    ipython.run_line_magic("load_ext", "autoreload")
    ipython.run_line_magic("autoreload", "2")

# Standard imports
import os
import torch
from tqdm.auto import tqdm

# Imports for displaying vis in Colab / notebook

torch.set_grad_enabled(False)

# For the most part I'll try to import functions and classes near where they are used
# to make it clear where they come from.

if torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Device: {device}")

def display_dashboard(
    sae_release="gpt2-small-res-jb",
    sae_id="blocks.7.hook_resid_pre",
    latent_idx=0,
    width=800,
    height=600,
):
    release = get_pretrained_saes_directory()[sae_release]
    neuronpedia_id = release.neuronpedia_id[sae_id]

    url = f"https://neuronpedia.org/{neuronpedia_id}/{latent_idx}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300"

    print(url)
    display(IFrame(url, width=width, height=height))

!pip install pandas numpy==1.26.0

from sae_lens import SAE, HookedSAETransformer
import os
os.environ['HF_TOKEN'] = ''

from sae_lens.loading.pretrained_saes_directory import get_pretrained_saes_directory
import pandas as pd
import plotly.express as px

# TODO: Make this nicer.
df = pd.DataFrame.from_records(
    {k: v.__dict__ for k, v in get_pretrained_saes_directory().items()}
).T
df.drop(
    columns=[
        "expected_var_explained",
        "expected_l0",
        "config_overrides",
        "conversion_func",
    ],
    inplace=True,
)

model = HookedSAETransformer.from_pretrained_no_processing("gpt2-small", device=device)

sae = SAE.from_pretrained(
    release="gpt2-small-res-jb",  # <- Release name
    sae_id="blocks.7.hook_resid_pre",  # <- SAE id (not always a hook point!)
    device=device
)

prompt = "the cat sat on the "
answer = "mat"
logits, cache = model.run_with_cache_with_saes(prompt, saes=[sae])

model.to_str_tokens(prompt)

"""is max activating examples the max activating examples for the latent.. or is it for the feature in response to the sentence.."""

import random
from IPython.display import IFrame, display

latent_idx = random.randint(0, sae.cfg.d_sae)
display_dashboard(latent_idx=latent_idx)

sae.error_term = False

"""The whole point of HookedSAETransformer is being able to splice in SAEs, replace model activations with their SAE reconstructions. So you just run a forward pass with SAEs attached.


This creates some sort of SAE-hooked forward pass
"""

from transformer_lens.utils import test_prompt
prompt = "Mitigating the risk of extinction from AI should be a global"
answer = " priority"

test_prompt(prompt, answer, model)

logits = model(prompt, return_type="logits")
logits_sae = model.run_with_saes(prompt, saes=[sae])
answer_token_id = model.to_single_token(answer)
top_prob, token_id_prediction = logits[0, -1].softmax(-1).max(-1)
top_prob_sae, token_id_prediction_sae = logits_sae[0, -1].softmax(-1).max(-1)
print(f"""Standard model:
    top prediction = {model.to_string(token_id_prediction)!r}
    prob = {top_prob.item():.2%}
SAE reconstruction:
    top prediction = {model.to_string(token_id_prediction_sae)!r}
    prob = {top_prob_sae.item():.2%}
""")

# getting activations
tokens = model.to_tokens(prompt)
logits, cache = model.run_with_cache_with_saes(tokens, saes=[sae])

for name, param in cache.items():
  if "hook_sae" in name:
    print(name, param)

# all this does is stop at the layer after the sae hook fires so that we get real values
# after that its just wasting compute. nothing downstream can change the sae activations
_, cache = model.run_with_cache_with_saes(prompt, saes=[sae])

# each key corresponds to a hook, and each value is the tensor captured at that hook
# cache[hook name][batch, seq_len, n_features] - which feature activated for the last token in the sae for batch 0
sae_acts_post = cache[f"{sae.cfg.metadata.hook_name}.hook_sae_acts_post"][0, -1, :]

sae.cfg

px.line(
    sae_acts_post.cpu().numpy(),
    title=f"Latent activations at the final token position ({sae_acts_post.nonzero().numel()} alive)",
    labels={"index": "Latent", "value": "Activation"},
    width=1000,
).update_layout(showlegend=False).show()
# Print the top 5 latents, and inspect their dashboards
for act, ind in zip(*sae_acts_post.topk(3)):
    print(f"Latent {ind} had activation {act:.2f}")
    display_dashboard(latent_idx=ind)

def get_activated_features(prompt:str):
  logits, cache = model.run_with_cache_with_saes(prompt, saes=[sae])
  top_logit_token_id = logits[0, -1].argmax(-1)
  top_logit_token_text = model.to_string(top_logit_token_id)
  cachename = f"{sae.cfg.metadata.hook_name}.hook_sae_acts_post"
  sae_acts_post = cache[cachename][0, -1, :]
  return {"token": top_logit_token_text, "latents": ([(act, idx) for act, idx in zip(*sae_acts_post.topk(5))])}

get_activated_features("the cat sat on the mat and ")

import requests

def get_autointerp_df(
    sae_release="gpt2-small-res-jb", sae_id="blocks.7.hook_resid_pre"
) -> pd.DataFrame:
    release = get_pretrained_saes_directory()[sae_release]
    neuronpedia_id = release.neuronpedia_id[sae_id]

    url = "https://www.neuronpedia.org/api/explanation/export?modelId={}&saeId={}".format(
        *neuronpedia_id.split("/")
    )
    headers = {"Content-Type": "application/json"}
    response = requests.get(url, headers=headers)

    data = response.json()
    return pd.DataFrame(data)


explanations_df = get_autointerp_df()
explanations_df.head()

def get_autointerp_explanations(prompt: str):
  df = get_autointerp_df()
  df['index'] = df['index'].astype(int)

  for act, idx in get_activated_features(prompt)['latents']:

    feature_explanation = df[df['index'] == idx.item()]
    if not feature_explanation.empty:
      print(f"Explanation for Feature {idx.item()} (Activation: {act:.2f}):")
      print(feature_explanation)
      print("\n" + "-"*50 + "\n") # Separator for readability
    else:
      print(f"No explanation found for Feature {idx.item()} (Activation: {act:.2f}).\n" + "-"*50 + "\n")

get_autointerp_explanations("ai poses extinction")

explanations_df.head()

def find_max_activation(model, sae, prompt, feature_idx):
    max_activation = 0.0

    tokens = model.to_tokens(prompt)
    layer = int(re.search(r"\.(\d+)\.", sae.cfg.metadata.hook_name).group(1))  # type: ignore
    _, cache = model.run_with_cache(
        tokens,
        stop_at_layer=layer + 1,
        names_filter=[sae.cfg.metadata.hook_name],
    )
    sae_in = cache[sae.cfg.metadata.hook_name]
    feature_acts = sae.encode(sae_in).squeeze(0)
    # feature_acts = feature_acts.flatten(0, 1)
    return feature_acts[:, feature_idx].max().item()

find_max_activation(model, sae, "the beaver knows how to build a ", 1223)

trust_remote_code=True

from tqdm.auto import tqdm
from functools import partial
import re

# instantiate an object to hold activations from a dataset
from sae_lens import ActivationsStore

from datasets import load_dataset
dataset = load_dataset(
    sae.cfg.metadata.dataset_path,
    split="train",
    trust_remote_code=True
)

# a convenient way to instantiate an activation store is to use the from_sae method
activation_store = ActivationsStore.from_sae(
    model=model,
    dataset=dataset,
    sae=sae,
    streaming=True,
    # fairly conservative parameters here so can use same for larger
    # models without running out of memory.
    store_batch_size_prompts=8,
    train_batch_size_tokens=4096,
    n_batches_in_buffer=32,
    device=device,
)

def steering(
    activations, hook, steering_strength=1.0, steering_vector=None, max_act=1.0
):
    return activations + max_act * steering_strength * steering_vector


def generate_with_steering(
    model,
    sae,
    prompt,
    steering_feature,
    max_act,
    steering_strength=1.0,
    max_new_tokens=95,
):
    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.metadata.prepend_bos)

    steering_vector = sae.W_dec[steering_feature].to(model.cfg.device)

    steering_hook = partial(
        steering,
        steering_vector=steering_vector,
        steering_strength=steering_strength,
        max_act=max_act,
    )

    with model.hooks(fwd_hooks=[(sae.cfg.metadata.hook_name, steering_hook)]):
        output = model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            stop_at_eos=False if device == "mps" else True,
            prepend_bos=sae.cfg.metadata.prepend_bos
        )

    return model.tokenizer.decode(output[0])


steering_feature = steering_feature = 20115  # Choose a feature to steer towards

# Find the maximum activation for this feature
max_act = find_max_activation(model, sae, activation_store, steering_feature)
print(f"Maximum activation for feature {steering_feature}: {max_act:.4f}")

# note we could also get the max activation from Neuronpedia (https://www.neuronpedia.org/api-doc#tag/lookup/GET/api/feature/{modelId}/{layer}/{index})

# Generate text without steering for comparison
prompt = "Once upon a time"
normal_text = model.generate(
    prompt,
    max_new_tokens=95,
    stop_at_eos=False if device == "mps" else True,
    prepend_bos=sae.cfg.metadata.prepend_bos,
)

print("\nNormal text (without steering):")
print(normal_text)

# Generate text with steering
steered_text = generate_with_steering(
    model, sae, prompt, steering_feature, max_act, steering_strength=2.0
)
print("Steered text:")
print(steered_text)

# from dataclasses import dataclass
# from functools import partial
# from typing import Any, Literal, NamedTuple, Callable

# import torch
# from sae_lens import SAE
# from transformer_lens import HookedTransformer
# from transformer_lens.hook_points import HookPoint


# class SaeReconstructionCache(NamedTuple):
#     sae_in: torch.Tensor
#     feature_acts: torch.Tensor
#     sae_out: torch.Tensor
#     sae_error: torch.Tensor


# def track_grad(tensor: torch.Tensor) -> None:
#     """wrapper around requires_grad and retain_grad"""
#     tensor.requires_grad_(True)
#     tensor.retain_grad()


# @dataclass
# class ApplySaesAndRunOutput:
#     model_output: torch.Tensor
#     model_activations: dict[str, torch.Tensor]
#     sae_activations: dict[str, SaeReconstructionCache]

#     def zero_grad(self) -> None:
#         """Helper to zero grad all tensors in this object."""
#         self.model_output.grad = None
#         for act in self.model_activations.values():
#             act.grad = None
#         for cache in self.sae_activations.values():
#             cache.sae_in.grad = None
#             cache.feature_acts.grad = None
#             cache.sae_out.grad = None
#             cache.sae_error.grad = None


# def apply_saes_and_run(
#     model: HookedTransformer,
#     saes: dict[str, SAE],
#     input: Any,
#     include_error_term: bool = True,
#     track_model_hooks: list[str] | None = None,
#     return_type: Literal["logits", "loss"] = "logits",
#     track_grads: bool = False,
# ) -> ApplySaesAndRunOutput:
#     """
#     Apply the SAEs to the model at the specific hook points, and run the model.
#     By default, this will include a SAE error term which guarantees that the SAE
#     will not affect model output. This function is designed to work correctly with
#     backprop as well, so it can be used for gradient-based feature attribution.

#     Args:
#         model: the model to run
#         saes: the SAEs to apply
#         input: the input to the model
#         include_error_term: whether to include the SAE error term to ensure the SAE doesn't affect model output. Default True
#         track_model_hooks: a list of hook points to record the activations and gradients. Default None
#         return_type: this is passed to the model.run_with_hooks function. Default "logits"
#         track_grads: whether to track gradients. Default False
#     """

#     fwd_hooks = []
#     bwd_hooks = []

#     sae_activations: dict[str, SaeReconstructionCache] = {}
#     model_activations: dict[str, torch.Tensor] = {}

#     # this hook just track the SAE input, output, features, and error. If `track_grads=True`, it also ensures
#     # that requires_grad is set to True and retain_grad is called for intermediate values.
#     def reconstruction_hook(sae_in: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001
#         sae = saes[hook_point]
#         feature_acts = sae.encode(sae_in)
#         sae_out = sae.decode(feature_acts)
#         sae_error = (sae_in - sae_out).detach().clone()
#         if track_grads:
#             track_grad(sae_error)
#             track_grad(sae_out)
#             track_grad(feature_acts)
#             track_grad(sae_in)
#         sae_activations[hook_point] = SaeReconstructionCache(
#             sae_in=sae_in,
#             feature_acts=feature_acts,
#             sae_out=sae_out,
#             sae_error=sae_error,
#         )

#         if include_error_term:
#             return sae_out + sae_error
#         return sae_out

#     def sae_bwd_hook(output_grads: torch.Tensor, hook: HookPoint):  # noqa: ARG001
#         # this just passes the output grads to the input, so the SAE gets the same grads despite the error term hackery
#         return (output_grads,)

#     # this hook just records model activations, and ensures that intermediate activations have gradient tracking turned on if needed
#     def tracking_hook(hook_input: torch.Tensor, hook: HookPoint, hook_point: str):  # noqa: ARG001
#         model_activations[hook_point] = hook_input
#         if track_grads:
#             track_grad(hook_input)
#         return hook_input

#     for hook_point in saes.keys():
#         fwd_hooks.append(
#             (hook_point, partial(reconstruction_hook, hook_point=hook_point))
#         )
#         bwd_hooks.append((hook_point, sae_bwd_hook))
#     for hook_point in track_model_hooks or []:
#         fwd_hooks.append((hook_point, partial(tracking_hook, hook_point=hook_point)))

#     # now, just run the model while applying the hooks
#     with model.hooks(fwd_hooks=fwd_hooks, bwd_hooks=bwd_hooks):
#         model_output = model(input, return_type=return_type)

#     return ApplySaesAndRunOutput(
#         model_output=model_output,
#         model_activations=model_activations,
#         sae_activations=sae_activations,
#     )
# from dataclasses import dataclass
# from transformer_lens.hook_points import HookPoint
# from dataclasses import dataclass
# from functools import partial
# from typing import Any, Literal, NamedTuple

# import torch
# from sae_lens import SAE
# from transformer_lens import HookedTransformer
# from transformer_lens.hook_points import HookPoint

# EPS = 1e-8

# torch.set_grad_enabled(True)


# @dataclass
# class AttributionGrads:
#     metric: torch.Tensor
#     model_output: torch.Tensor
#     model_activations: dict[str, torch.Tensor]
#     sae_activations: dict[str, SaeReconstructionCache]


# @dataclass
# class Attribution:
#     model_attributions: dict[str, torch.Tensor]
#     model_activations: dict[str, torch.Tensor]
#     model_grads: dict[str, torch.Tensor]
#     sae_feature_attributions: dict[str, torch.Tensor]
#     sae_feature_activations: dict[str, torch.Tensor]
#     sae_feature_grads: dict[str, torch.Tensor]
#     sae_errors_attribution_proportion: dict[str, float]


# def calculate_attribution_grads(
#     model: HookedSAETransformer,
#     prompt: str,
#     metric_fn: Callable[[torch.Tensor], torch.Tensor],
#     track_hook_points: list[str] | None = None,
#     include_saes: dict[str, SAE] | None = None,
#     return_logits: bool = True,
#     include_error_term: bool = True,
# ) -> AttributionGrads:
#     """
#     Wrapper around apply_saes_and_run that calculates gradients wrt to the metric_fn.
#     Tracks grads for both SAE feature and model neurons, and returns them in a structured format.
#     """
#     output = apply_saes_and_run(
#         model,
#         saes=include_saes or {},
#         input=prompt,
#         return_type="logits" if return_logits else "loss",
#         track_model_hooks=track_hook_points,
#         include_error_term=include_error_term,
#         track_grads=True,
#     )
#     metric = metric_fn(output.model_output)
#     output.zero_grad()
#     metric.backward()
#     return AttributionGrads(
#         metric=metric,
#         model_output=output.model_output,
#         model_activations=output.model_activations,
#         sae_activations=output.sae_activations,
#     )


# def calculate_feature_attribution(
#     model: HookedSAETransformer,
#     input: Any,
#     metric_fn: Callable[[torch.Tensor], torch.Tensor],
#     track_hook_points: list[str] | None = None,
#     include_saes: dict[str, SAE] | None = None,
#     return_logits: bool = True,
#     include_error_term: bool = True,
# ) -> Attribution:
#     """
#     Calculate feature attribution for SAE features and model neurons following
#     the procedure in https://transformer-circuits.pub/2024/march-update/index.html#feature-heads.
#     This include the SAE error term by default, so inserting the SAE into the calculation is
#     guaranteed to not affect the model output. This can be disabled by setting `include_error_term=False`.

#     Args:
#         model: The model to calculate feature attribution for.
#         input: The input to the model.
#         metric_fn: A function that takes the model output and returns a scalar metric.
#         track_hook_points: A list of model hook points to track activations for, if desired
#         include_saes: A dictionary of SAEs to include in the calculation. The key is the hook point to apply the SAE to.
#         return_logits: Whether to return the model logits or loss. This is passed to TLens, so should match whatever the metric_fn expects (probably logits)
#         include_error_term: Whether to include the SAE error term in the calculation. This is recommended, as it ensures that the SAE will not affecting the model output.
#     """
#     # first, calculate gradients wrt to the metric_fn.
#     # these will be multiplied with the activation values to get the attributions
#     outputs_with_grads = calculate_attribution_grads(
#         model,
#         input,
#         metric_fn,
#         track_hook_points,
#         include_saes=include_saes,
#         return_logits=return_logits,
#         include_error_term=include_error_term,
#     )
#     model_attributions = {}
#     model_activations = {}
#     model_grads = {}
#     sae_feature_attributions = {}
#     sae_feature_activations = {}
#     sae_feature_grads = {}
#     sae_error_proportions = {}
#     # this code is long, but all it's doing is multiplying the grads by the activations
#     # and recording grads, acts, and attributions in dictionaries to return to the user
#     with torch.no_grad():
#         for name, act in outputs_with_grads.model_activations.items():
#             assert act.grad is not None
#             raw_activation = act.detach().clone()
#             model_attributions[name] = (act.grad * raw_activation).detach().clone()
#             model_activations[name] = raw_activation
#             model_grads[name] = act.grad.detach().clone()
#         for name, act in outputs_with_grads.sae_activations.items():
#             assert act.feature_acts.grad is not None
#             assert act.sae_out.grad is not None
#             raw_activation = act.feature_acts.detach().clone()
#             sae_feature_attributions[name] = (
#                 (act.feature_acts.grad * raw_activation).detach().clone()
#             )
#             sae_feature_activations[name] = raw_activation
#             sae_feature_grads[name] = act.feature_acts.grad.detach().clone()
#             if include_error_term:
#                 assert act.sae_error.grad is not None
#                 error_grad_norm = act.sae_error.grad.norm().item()
#             else:
#                 error_grad_norm = 0
#             sae_out_norm = act.sae_out.grad.norm().item()
#             sae_error_proportions[name] = error_grad_norm / (
#                 sae_out_norm + error_grad_norm + EPS
#             )
#         return Attribution(
#             model_attributions=model_attributions,
#             model_activations=model_activations,
#             model_grads=model_grads,
#             sae_feature_attributions=sae_feature_attributions,
#             sae_feature_activations=sae_feature_activations,
#             sae_feature_grads=sae_feature_grads,
#             sae_errors_attribution_proportion=sae_error_proportions,
#         )


# # prompt = " Tiger Woods plays the sport of"
# # pos_token = model.tokenizer.encode(" golf")[0]
# prompt = "In the beginning, God created the heavens and the"
# pos_token = model.tokenizer.encode(" earth")
# neg_token = model.tokenizer.encode(" sky")


# def metric_fn(
#     logits: torch.tensor,
#     pos_token: torch.tensor = pos_token,
#     neg_token: torch.Tensor = neg_token,
# ) -> torch.Tensor:
#     return logits[0, -1, pos_token] - logits[0, -1, neg_token]


# feature_attribution_df = calculate_feature_attribution(
#     input=prompt,
#     model=model,
#     metric_fn=metric_fn,
#     include_saes={sae.cfg.metadata.hook_name: sae},
#     include_error_term=True,
#     return_logits=True,
# )
# from transformer_lens.utils import test_prompt

# test_prompt(prompt, model.to_string(pos_token), model)
# tokens = model.to_str_tokens(prompt)
# unique_tokens = [f"{i}/{t}" for i, t in enumerate(tokens)]

# px.bar(
#     x=unique_tokens,
#     y=feature_attribution_df.sae_feature_attributions[sae.cfg.metadata.hook_name][0]
#     .sum(-1)
#     .detach()
#     .cpu()
#     .numpy(),
# )
# def convert_sparse_feature_to_long_df(sparse_tensor: torch.Tensor) -> pd.DataFrame:
#     """
#     Convert a sparse tensor to a long format pandas DataFrame.
#     """
#     df = pd.DataFrame(sparse_tensor.detach().cpu().numpy())
#     df_long = df.melt(ignore_index=False, var_name="column", value_name="value")
#     df_long.columns = ["feature", "attribution"]
#     df_long_nonzero = df_long[df_long["attribution"] != 0]
#     df_long_nonzero = df_long_nonzero.reset_index().rename(
#         columns={"index": "position"}
#     )
#     return df_long_nonzero


# df_long_nonzero = convert_sparse_feature_to_long_df(
#     feature_attribution_df.sae_feature_attributions[sae.cfg.metadata.hook_name][0]
# )
# df_long_nonzero.sort_values("attribution", ascending=False)
# for i, v in (
#     df_long_nonzero.query("position==8")
#     .groupby("feature")
#     .attribution.sum()
#     .sort_values(ascending=False)
#     .head(5)
#     .items()
# ):
#     print(f"Feature {i} had a total attribution of {v:.2f}")
#     html = get_dashboard_html(
#         sae_release="gpt2-small",
#         sae_id=f"{extract_layer_from_tlens_hook_name(sae.cfg.metadata.hook_name)}-res-jb",
#         feature_idx=int(i),
#     )
#     display(IFrame(html, width=1200, height=300))
#     for i, v in (
#     df_long_nonzero.groupby("feature")
#     .attribution.sum()
#     .sort_values(ascending=False)
#     .head(5)
#     .items()
# ):
#     print(f"Feature {i} had a total attribution of {v:.2f}")
#     html = get_dashboard_html(
#         sae_release="gpt2-small",
#         sae_id=f"{extract_layer_from_tlens_hook_name(sae.cfg.metadata.hook_name)}-res-jb",
#         feature_idx=int(i),
#     )
#     display(IFrame(html, width=1200, height=300))
